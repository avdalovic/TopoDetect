{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "import numpy as np\n",
    "import toponetx as tnx\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from topomodelx.nn.combinatorial.hmc import HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHRECDataset(Dataset):\n",
    "    \"\"\"Class for the SHREC 2016 dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : npz file\n",
    "        npz file containing the SHREC 2016 data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data) -> None:\n",
    "        self.complexes = [cc.to_combinatorial_complex() for cc in data[\"complexes\"]]\n",
    "        self.x_0 = data[\"node_feat\"]\n",
    "        self.x_1 = data[\"edge_feat\"]\n",
    "        self.x_2 = data[\"face_feat\"]\n",
    "        self.y = data[\"label\"]\n",
    "        self.a0, self.a1, self.coa2, self.b1, self.b2 = self._get_neighborhood_matrix()\n",
    "\n",
    "    def _get_neighborhood_matrix(self) -> list[list[torch.sparse.Tensor], ...]:\n",
    "        \"\"\"Neighborhood matrices for each combinatorial complex in the dataset.\n",
    "\n",
    "        Following the Higher Order Attention Model for Mesh Classification message passing scheme, this method computes the necessary neighborhood matrices\n",
    "        for each combinatorial complex in the dataset. This method computes:\n",
    "\n",
    "        - Adjacency matrices for each 0-cell in the dataset.\n",
    "        - Adjacency matrices for each 1-cell in the dataset.\n",
    "        - Coadjacency matrices for each 2-cell in the dataset.\n",
    "        - Incidence matrices from 1-cells to 0-cells for each 1-cell in the dataset.\n",
    "        - Incidence matrices from 2-cells to 1-cells for each 2-cell in the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a0 : list of torch.sparse.FloatTensor\n",
    "            Adjacency matrices for each 0-cell in the dataset.\n",
    "        a1 : list of torch.sparse.FloatTensor\n",
    "            Adjacency matrices for each 1-cell in the dataset.\n",
    "        coa2 : list of torch.sparse.FloatTensor\n",
    "            Coadjacency matrices for each 2-cell in the dataset.\n",
    "        b1 : list of torch.sparse.FloatTensor\n",
    "            Incidence matrices from 1-cells to 0-cells for each 1-cell in the dataset.\n",
    "        b2 : list of torch.sparse.FloatTensor\n",
    "            Incidence matrices from 2-cells to 1-cells for each 2-cell in the dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        a0 = []\n",
    "        a1 = []\n",
    "        coa2 = []\n",
    "        b1 = []\n",
    "        b2 = []\n",
    "\n",
    "        for cc in self.complexes:\n",
    "            a0.append(torch.from_numpy(cc.adjacency_matrix(0, 1).todense()).to_sparse())\n",
    "            a1.append(torch.from_numpy(cc.adjacency_matrix(1, 2).todense()).to_sparse())\n",
    "\n",
    "            B = cc.incidence_matrix(rank=1, to_rank=2)\n",
    "            A = B.T @ B\n",
    "            A.setdiag(0)\n",
    "            coa2.append(torch.from_numpy(A.todense()).to_sparse())\n",
    "\n",
    "            b1.append(torch.from_numpy(cc.incidence_matrix(0, 1).todense()).to_sparse())\n",
    "            b2.append(torch.from_numpy(cc.incidence_matrix(1, 2).todense()).to_sparse())\n",
    "\n",
    "        return a0, a1, coa2, b1, b2\n",
    "\n",
    "    def num_classes(self) -> int:\n",
    "        \"\"\"Returns the number of classes in the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Number of classes in the dataset.\n",
    "        \"\"\"\n",
    "        return len(np.unique(self.y))\n",
    "\n",
    "    def channels_dim(self) -> tuple[int, int, int]:\n",
    "        \"\"\"Returns the number of channels for each input signal.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of int\n",
    "            Number of channels for each input signal.\n",
    "        \"\"\"\n",
    "        return [self.x_0[0].shape[1], self.x_1[0].shape[1], self.x_2[0].shape[1]]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the number of elements in the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Number of elements in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.complexes)\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple[torch.Tensor, ...]:\n",
    "        \"\"\"Returns the idx-th element in the dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Index of the element to return.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of torch.Tensor\n",
    "            Tuple containing the idx-th element in the dataset, including the input signals on nodes, edges and faces, the neighborhood matrices and the label.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.x_0[idx],\n",
    "            self.x_1[idx],\n",
    "            self.x_2[idx],\n",
    "            self.a0[idx],\n",
    "            self.a1[idx],\n",
    "            self.coa2[idx],\n",
    "            self.b1[idx],\n",
    "            self.b2[idx],\n",
    "            self.y[idx],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shrec_training, shrec_testing = tnx.datasets.shrec_16()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SHREC 2016 dataset...\n",
      "\n",
      "=== SHREC Dataset Overview ===\n",
      "Training set size: 480\n",
      "Testing set size: 120\n",
      "\n",
      "=== First Training Complex ===\n",
      "Type: <class 'toponetx.classes.simplicial_complex.SimplicialComplex'>\n",
      "\n",
      "=== Complex Structure ===\n",
      "Number of 0-simplices (nodes): 252\n",
      "Number of 1-simplices (edges): 498\n",
      "Number of 2-simplices (faces): -250\n",
      "\n",
      "=== Feature Information ===\n",
      "Node features shape: (252, 6)\n",
      "Edge features shape: (750, 10)\n",
      "Face features shape: (500, 7)\n",
      "\n",
      "=== Label Information ===\n",
      "First complex label: 0\n",
      "Unique labels: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "Label distribution: [(0, 16), (1, 16), (2, 16), (3, 16), (4, 16), (5, 16), (6, 16), (7, 16), (8, 16), (9, 16), (10, 16), (11, 16), (12, 16), (13, 16), (14, 16), (15, 16), (16, 16), (17, 16), (18, 16), (19, 16), (20, 16), (21, 16), (22, 16), (23, 16), (24, 16), (25, 16), (26, 16), (27, 16), (28, 16), (29, 16)]\n",
      "\n",
      "=== Example Cell Features ===\n",
      "First node features: [ 0.567542    0.570995   -0.210023   -0.06894332  0.72564355  0.6846081 ]\n",
      "First edge features: [1.22957341 0.13581241 0.45991269 0.42847896 0.468195   0.51764337\n",
      " 2.25383585 2.25198965 2.89552337 2.00058261]\n",
      "First face features: [6.16026555e-04 3.42332662e-01 9.39444198e-01 1.59043318e-02\n",
      " 9.55859971e-01 4.28478956e-01 1.75725373e+00]\n",
      "\n",
      "=== Key-Index Mappings ===\n",
      "Rank 0 cell indices (first 5):\n",
      "  0: frozenset({'1'})\n",
      "  1: frozenset({'0'})\n",
      "  2: frozenset({'11'})\n",
      "  3: frozenset({'12'})\n",
      "  4: frozenset({'9'})\n",
      "\n",
      "Rank 1 cell indices (first 5):\n",
      "  0: frozenset({'1', '0'})\n",
      "  1: frozenset({'11', '0'})\n",
      "  2: frozenset({'12', '0'})\n",
      "  3: frozenset({'9', '0'})\n",
      "  4: frozenset({'11', '1'})\n",
      "\n",
      "Rank 2 cell indices (first 5):\n",
      "  0: frozenset({'11', '1', '0'})\n",
      "  1: frozenset({'1', '9', '0'})\n",
      "  2: frozenset({'11', '12', '0'})\n",
      "  3: frozenset({'12', '9', '0'})\n",
      "  4: frozenset({'11', '1', '14'})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import toponetx as tnx\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Load the SHREC 2016 dataset\n",
    "print(\"Loading SHREC 2016 dataset...\")\n",
    "shrec_training, shrec_testing = tnx.datasets.shrec_16()\n",
    "\n",
    "# Basic dataset information\n",
    "print(\"\\n=== SHREC Dataset Overview ===\")\n",
    "print(f\"Training set size: {len(shrec_training['complexes'])}\")\n",
    "print(f\"Testing set size: {len(shrec_testing['complexes'])}\")\n",
    "\n",
    "# Examining the first complex\n",
    "print(\"\\n=== First Training Complex ===\")\n",
    "first_complex = shrec_training['complexes'][0]\n",
    "print(f\"Type: {type(first_complex)}\")\n",
    "\n",
    "# Basic complex statistics for SimplicialComplex\n",
    "print(\"\\n=== Complex Structure ===\")\n",
    "# For SimplicialComplex, we access the simplices by dimension\n",
    "print(f\"Number of 0-simplices (nodes): {len(list(first_complex.skeleton(0)))}\")\n",
    "print(f\"Number of 1-simplices (edges): {len(list(first_complex.skeleton(1))) - len(list(first_complex.skeleton(0)))}\")\n",
    "print(f\"Number of 2-simplices (faces): {len(list(first_complex.skeleton(2))) - len(list(first_complex.skeleton(1)))}\")\n",
    "\n",
    "# Display feature information\n",
    "print(\"\\n=== Feature Information ===\")\n",
    "print(f\"Node features shape: {shrec_training['node_feat'][0].shape}\")\n",
    "print(f\"Edge features shape: {shrec_training['edge_feat'][0].shape}\")\n",
    "print(f\"Face features shape: {shrec_training['face_feat'][0].shape}\")\n",
    "\n",
    "# Display label information\n",
    "print(\"\\n=== Label Information ===\")\n",
    "print(f\"First complex label: {shrec_training['label'][0]}\")\n",
    "unique_labels = np.unique(shrec_training['label'])\n",
    "print(f\"Unique labels: {unique_labels}\")\n",
    "print(f\"Label distribution: {[(label, np.sum(shrec_training['label'] == label)) for label in unique_labels]}\")\n",
    "\n",
    "\n",
    "\n",
    "# Example of extracting cell features\n",
    "print(\"\\n=== Example Cell Features ===\")\n",
    "node_feats = shrec_training['node_feat'][0]\n",
    "edge_feats = shrec_training['edge_feat'][0]\n",
    "face_feats = shrec_training['face_feat'][0]\n",
    "\n",
    "print(f\"First node features: {node_feats[0]}\")\n",
    "print(f\"First edge features: {edge_feats[0]}\")\n",
    "print(f\"First face features: {face_feats[0]}\")\n",
    "\n",
    "# Show first few entries in each dictionary with their corresponding indices\n",
    "print(\"\\n=== Key-Index Mappings ===\")\n",
    "try:\n",
    "    # Convert to combinatorial complex for index mapping\n",
    "    cc = first_complex.to_combinatorial_complex()\n",
    "    \n",
    "    # Get incidence matrices with indices\n",
    "    row, column, B1 = cc.incidence_matrix(0, 1, index=True)\n",
    "    row1, column1, B2 = cc.incidence_matrix(1, 2, index=True)\n",
    "    \n",
    "    # Print first few entries of each dictionary\n",
    "    print(\"Rank 0 cell indices (first 5):\")\n",
    "    for i, (cell, idx) in enumerate(row.items()):\n",
    "        if i >= 5: break\n",
    "        print(f\"  {idx}: {cell}\")\n",
    "    \n",
    "    print(\"\\nRank 1 cell indices (first 5):\")\n",
    "    for i, (cell, idx) in enumerate(column.items()):\n",
    "        if i >= 5: break\n",
    "        print(f\"  {idx}: {cell}\")\n",
    "    \n",
    "    print(\"\\nRank 2 cell indices (first 5):\")\n",
    "    for i, (cell, idx) in enumerate(column1.items()):\n",
    "        if i >= 5: break\n",
    "        print(f\"  {idx}: {cell}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not convert to combinatorial complex: {e}\")\n",
    "    \n",
    "    # Alternative: just show node positions\n",
    "    print(\"Node positions (first 5):\")\n",
    "    nodes = list(first_complex.nodes)\n",
    "    for i, node in enumerate(nodes[:5]):\n",
    "        print(f\"  Node {i}: {node}\")\n",
    "    \n",
    "    # Show edges\n",
    "    print(\"\\nEdges (first 5):\")\n",
    "    edges = [e for e in first_complex.skeleton(1) if len(e) == 2]\n",
    "    for i, edge in enumerate(edges[:5]):\n",
    "        print(f\"  Edge {i}: {edge}\")\n",
    "    \n",
    "    # Show faces\n",
    "    print(\"\\nFaces (first 5):\")\n",
    "    faces = [f for f in first_complex.skeleton(2) if len(f) == 3]\n",
    "    for i, face in enumerate(faces[:5]):\n",
    "        print(f\"  Face {i}: {face}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = SHRECDataset(shrec_training)\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = SHRECDataset(shrec_testing)\n",
    "testing_dataloader = DataLoader(testing_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"Trainer for the HOANMeshClassifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The model to train.\n",
    "    training_dataloader : torch.utils.data.DataLoader\n",
    "        The dataloader for the training set.\n",
    "    testing_dataloader : torch.utils.data.DataLoader\n",
    "        The dataloader for the testing set.\n",
    "    learning_rate : float\n",
    "        The learning rate for the Adam optimizer.\n",
    "    device : torch.device\n",
    "        The device to use for training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model, training_dataloader, testing_dataloader, learning_rate, device\n",
    "    ) -> None:\n",
    "        self.model = model.to(device)\n",
    "        self.training_dataloader = training_dataloader\n",
    "        self.testing_dataloader = testing_dataloader\n",
    "        self.device = device\n",
    "        self.crit = torch.nn.CrossEntropyLoss()\n",
    "        self.opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def _to_device(self, x) -> list[torch.Tensor]:\n",
    "        \"\"\"Converts tensors to the correct type and moves them to the device.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : List[torch.Tensor]\n",
    "            List of tensors to convert.\n",
    "        Returns\n",
    "        -------\n",
    "        List[torch.Tensor]\n",
    "            List of converted tensors to float type and moved to the device.\n",
    "        \"\"\"\n",
    "\n",
    "        return [el[0].float().to(self.device) for el in x]\n",
    "\n",
    "    def train(self, num_epochs=500, test_interval=25) -> None:\n",
    "        \"\"\"Trains the model for the specified number of epochs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_epochs : int\n",
    "            Number of epochs to train.\n",
    "        test_interval : int\n",
    "            Interval between testing epochs.\n",
    "        \"\"\"\n",
    "        for epoch_i in range(num_epochs):\n",
    "            training_accuracy, epoch_loss = self._train_epoch()\n",
    "            print(\n",
    "                f\"Epoch: {epoch_i} loss: {epoch_loss:.4f} Train_acc: {training_accuracy:.4f}\",\n",
    "                flush=True,\n",
    "            )\n",
    "            if (epoch_i + 1) % test_interval == 0:\n",
    "                test_accuracy = self.validate()\n",
    "                print(f\"Test_acc: {test_accuracy:.4f}\", flush=True)\n",
    "\n",
    "    def _train_epoch(self) -> tuple[float, float]:\n",
    "        \"\"\"Trains the model for one epoch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        training_accuracy : float\n",
    "            The mean training accuracy for the epoch.\n",
    "        epoch_loss : float\n",
    "            The mean loss for the epoch.\n",
    "        \"\"\"\n",
    "        training_samples = len(self.training_dataloader.dataset)\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        self.model.train()\n",
    "        for sample in self.training_dataloader:\n",
    "            (\n",
    "                x_0,\n",
    "                x_1,\n",
    "                x_2,\n",
    "                adjacency_0,\n",
    "                adjacency_1,\n",
    "                coadjacency_2,\n",
    "                incidence_1,\n",
    "                incidence_2,\n",
    "            ) = self._to_device(sample[:-1])\n",
    "\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "            y_hat = self.model.forward(\n",
    "                x_0,\n",
    "                x_1,\n",
    "                x_2,\n",
    "                adjacency_0,\n",
    "                adjacency_1,\n",
    "                coadjacency_2,\n",
    "                incidence_1,\n",
    "                incidence_2,\n",
    "            )\n",
    "\n",
    "            y = sample[-1][0].long().to(self.device)\n",
    "            total_loss += self._compute_loss_and_update(y_hat, y)\n",
    "            correct += (y_hat.argmax() == y).sum().item()\n",
    "\n",
    "        training_accuracy = correct / training_samples\n",
    "        epoch_loss = total_loss / training_samples\n",
    "\n",
    "        return training_accuracy, epoch_loss\n",
    "\n",
    "    def _compute_loss_and_update(self, y_hat, y) -> float:\n",
    "        \"\"\"Computes the loss, performs backpropagation, and updates the model's parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_hat : torch.Tensor\n",
    "            The output of the model.\n",
    "        y : torch.Tensor\n",
    "            The ground truth.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: float\n",
    "            The loss value.\n",
    "        \"\"\"\n",
    "\n",
    "        loss = self.crit(y_hat, y)\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def validate(self) -> float:\n",
    "        \"\"\"Validates the model using the testing dataloader.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        test_accuracy : float\n",
    "            The mean testing accuracy.\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        self.model.eval()\n",
    "        test_samples = len(self.testing_dataloader.dataset)\n",
    "        with torch.no_grad():\n",
    "            for sample in self.testing_dataloader:\n",
    "                (\n",
    "                    x_0,\n",
    "                    x_1,\n",
    "                    x_2,\n",
    "                    adjacency_0,\n",
    "                    adjacency_1,\n",
    "                    coadjacency_2,\n",
    "                    incidence_1,\n",
    "                    incidence_2,\n",
    "                ) = self._to_device(sample[:-1])\n",
    "\n",
    "                y_hat = self.model(\n",
    "                    x_0,\n",
    "                    x_1,\n",
    "                    x_2,\n",
    "                    adjacency_0,\n",
    "                    adjacency_1,\n",
    "                    coadjacency_2,\n",
    "                    incidence_1,\n",
    "                    incidence_2,\n",
    "                )\n",
    "                y = sample[-1][0].long().to(self.device)\n",
    "                correct += (y_hat.argmax() == y).sum().item()\n",
    "            return correct / test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels_per_layer,\n",
    "        negative_slope=0.2,\n",
    "        num_classes=2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.base_model = HMC(\n",
    "            channels_per_layer,\n",
    "            negative_slope,\n",
    "        )\n",
    "        self.l0 = torch.nn.Linear(channels_per_layer[-1][2][0], num_classes)\n",
    "        self.l1 = torch.nn.Linear(channels_per_layer[-1][2][1], num_classes)\n",
    "        self.l2 = torch.nn.Linear(channels_per_layer[-1][2][2], num_classes)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_0,\n",
    "        x_1,\n",
    "        x_2,\n",
    "        neighborhood_0_to_0,\n",
    "        neighborhood_1_to_1,\n",
    "        neighborhood_2_to_2,\n",
    "        neighborhood_0_to_1,\n",
    "        neighborhood_1_to_2,\n",
    "    ):\n",
    "        x_0, x_1, x_2 = self.base_model(\n",
    "            x_0,\n",
    "            x_1,\n",
    "            x_2,\n",
    "            neighborhood_0_to_0,\n",
    "            neighborhood_1_to_1,\n",
    "            neighborhood_2_to_2,\n",
    "            neighborhood_0_to_1,\n",
    "            neighborhood_1_to_2,\n",
    "        )\n",
    "        x_0 = self.l0(x_0)\n",
    "        x_1 = self.l1(x_1)\n",
    "        x_2 = self.l2(x_2)\n",
    "\n",
    "        # Sum all the elements in the dimension zero\n",
    "        x_0 = torch.nanmean(x_0, dim=0)\n",
    "        x_1 = torch.nanmean(x_1, dim=0)\n",
    "        x_2 = torch.nanmean(x_2, dim=0)\n",
    "\n",
    "        return x_0 + x_1 + x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = training_dataset.channels_dim()\n",
    "intermediate_channels = [60, 60, 60]\n",
    "final_channels = [60, 60, 60]\n",
    "\n",
    "channels_per_layer = [[in_channels, intermediate_channels, final_channels]]\n",
    "# defube HOAN mesh classifier\n",
    "model = Network(\n",
    "    channels_per_layer, negative_slope=0.2, num_classes=training_dataset.num_classes()\n",
    ")\n",
    "\n",
    "# If GPU's are available, we will make use of them. Otherwise, this will run on CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "trainer = Trainer(model, training_dataloader, testing_dataloader, 0.001, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
